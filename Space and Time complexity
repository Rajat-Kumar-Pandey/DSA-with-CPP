Time Complexity:

1.	O(1) - Constant Time
o	Independent of input size; the algorithm takes the same amount of time.
o	Example: Accessing an array element by index.

2.	O(log n) - Logarithmic Time
o	Cuts the problem size in half with each iteration.
o	Example: Binary Search.

3.	O(n) - Linear Time
o	Time grows directly proportional to the input size.
o	Example: Traversing an array.

4.	O(n log n) - Linearithmic Time
o	Performs linear operations for each split of the input.
o	Example: Merge Sort, Heap Sort.

5.	O(n²) - Quadratic Time
o	Time grows as the square of the input size.
o	Example: Bubble Sort, Selection Sort, Insertion Sort.

6.	O(n³) - Cubic Time
o	Time grows as the cube of the input size.
o	Example: Matrix Multiplication (naive approach).

7.	O(2^n) - Exponential Time
o	Time doubles with each additional input.
o	Example: Recursive solutions to the Fibonacci sequence, Subset generation.

8.	O(n!) - Factorial Time
o	Grows faster than exponential; used in problems involving permutations.
o	Example: Solving the Travelling Salesman Problem using brute force.

________________________________________
Space Complexity:

1.	O(1) - Constant Space
o	Uses a fixed amount of memory, regardless of input size.
o	Example: Swapping two numbers.

2.	O(log n) - Logarithmic Space
o	Recursive algorithms like Binary Search require this for stack space.
o	Example: Divide-and-conquer recursion.

3.	O(n) - Linear Space
o	Memory grows directly proportional to input size.
o	Example: Storing elements in an array.

4.	O(n log n) - Linearithmic Space
o	Memory required for sorting algorithms like Merge Sort.
o	Example: Temporary storage in divide-and-conquer algorithms.

5.	O(n²) - Quadratic Space
o	Often needed for dynamic programming with 2D tables.
o	Example: Longest Common Subsequence.

6.	O(2^n) - Exponential Space
o	Memory grows exponentially with input.
o	Example: Recursive Fibonacci with no memoization.

7.	O(n!) - Factorial Space
o	Extremely rare; used in combinatorial problems.
o	Example: Generating all permutations of a string or set.
________________________________________

Summary Table

Complexity	    Time (Best to Worst)	  Space (Best to Worst)
Constant	              O(1)	                  O(1)
Logarithmic	          O(log n)	            	O(log n)
Linear	                O(n)	                 	O(n)
Linearithmic	       O(n log n)            	O(n log n)
Quadratic	             O(n²)	                	O(n²)
Cubic	                  O(n³)                  	O(n³)
Exponential	           O(2^n)                	O(2^n)
Factorial	              O(n!)                  	O(n!)
________________________________________


The time and space complexity of an algorithm describes how its resource requirements change as the **input size (n)** increases. Here's an explanation of how different complexities behave as input size grows:

---

### **1. O(1) – Constant Time/Space**
- **Behavior**: Unaffected by input size. The time or space stays the same, regardless of how much the input grows.
- **Example**:
  - **Time**: Accessing the first element of an array: `arr[0]`.
  - **Space**: Using a few fixed variables: `int x, y;`.
- **Change with Input Size**: No change. It’s the most efficient.

---

### **2. O(log n) – Logarithmic Time/Space**
- **Behavior**: Increases very slowly as the input size grows. Typically occurs in algorithms that divide the problem in half repeatedly.
- **Example**:
  - **Time**: Binary search (input size halves with each step).
  - **Space**: Recursive algorithms like binary search, using logarithmic stack frames.
- **Change with Input Size**: 
  - For input size `n = 10`, takes ~3 steps.
  - For `n = 1000`, takes ~10 steps.
  - For `n = 1,000,000`, takes ~20 steps.

---

### **3. O(n) – Linear Time/Space**
- **Behavior**: Grows proportionally to input size. Doubling the input size doubles the time or space requirement.
- **Example**:
  - **Time**: Iterating through an array: `for i in range(n)`.
  - **Space**: Allocating a new array of size `n`.
- **Change with Input Size**:
  - If input grows from `n = 100` to `n = 200`, resource usage doubles.

---

### **4. O(n log n) – Linearithmic Time/Space**
- **Behavior**: Increases faster than linear but slower than quadratic. Common in efficient divide-and-conquer algorithms.
- **Example**:
  - **Time**: Merge Sort (splitting input repeatedly and merging).
  - **Space**: Merge Sort (temporary storage for merging).
- **Change with Input Size**:
  - For `n = 100`, requires ~700 units of time.
  - For `n = 1000`, requires ~10,000 units of time.
  - For `n = 1,000,000`, requires ~20,000,000 units of time.

---

### **5. O(n²) – Quadratic Time/Space**
- **Behavior**: Grows quickly as the square of the input size. Doubling the input size causes a fourfold increase in time or space.
- **Example**:
  - **Time**: Nested loops for comparing all pairs: `for i in range(n): for j in range(n)`.
  - **Space**: Dynamic programming with 2D tables, such as Longest Common Subsequence.
- **Change with Input Size**:
  - For `n = 10`, requires 100 units of time.
  - For `n = 100`, requires 10,000 units of time.
  - For `n = 1000`, requires 1,000,000 units of time.

---

### **6. O(n³) – Cubic Time/Space**
- **Behavior**: Grows very quickly. Doubling the input size increases time or space by 8x.
- **Example**:
  - **Time**: Triple nested loops: `for i in range(n): for j in range(n): for k in range(n)`.
  - **Space**: Complex 3D data storage.
- **Change with Input Size**:
  - For `n = 10`, requires 1,000 units of time.
  - For `n = 100`, requires 1,000,000 units of time.
  - For `n = 1000`, requires 1,000,000,000 units of time.

---

### **7. O(2ⁿ) – Exponential Time/Space**
- **Behavior**: Grows extremely rapidly. Adding a single unit to the input size doubles the time or space required.
- **Example**:
  - **Time**: Recursive solutions like the naive Fibonacci sequence.
  - **Space**: Each recursive call requires its own stack space.
- **Change with Input Size**:
  - For `n = 10`, requires 1024 units of time.
  - For `n = 20`, requires 1,048,576 units of time.
  - For `n = 30`, requires 1,073,741,824 units of time.

---

### **8. O(n!) – Factorial Time/Space**
- **Behavior**: Grows faster than exponential. Resource requirements become unmanageable even for small inputs.
- **Example**:
  - **Time**: Generating all permutations of a set.
  - **Space**: Storing permutations or recursive calls for permutations.
- **Change with Input Size**:
  - For `n = 5`, requires 120 units of time.
  - For `n = 10`, requires 3,628,800 units of time.
  - For `n = 20`, requires ~2.4 × 10¹⁸ units of time.

---

### **Visual Growth Comparison**
- **Slow growth**: O(1), O(log n), O(n)
- **Moderate growth**: O(n log n), O(n²)
- **Fast growth**: O(2ⁿ), O(n!)

| Input Size (n) | O(1) | O(log n) | O(n)   | O(n log n)   | O(n²)   | O(2ⁿ)    | O(n!) |
|----------------|------|----------|--------|--------------|---------|----------|-------|
| 1              | 1     | 0       | 1      | 0            | 1       | 2        | 1     |
| 10             | 1     | 3       | 10     | 33           | 100     | 1024     | 3.6M  |
| 100            | 1     | 6       | 100    | 664          | 10K     | ~10³⁰    | Huge  |

---

### **How It Impacts Algorithms**
- **Small Input Sizes**: Complex algorithms (e.g., O(n²), O(2ⁿ)) may still perform well.
- **Large Input Sizes**: Efficient algorithms (e.g., O(log n), O(n)) are critical to prevent resource overload.
- **Trade-offs**: Sometimes, you compromise space for time (e.g., using memoization).


